{
    "batch_size":[64],

    "log_learning_rate": [-4, -2],

    "activation": ["relu", "tanh", "silu"],

    "hidden_size":[0.5, 2],

    "mlp_layers": [1, 4],

    "alpha": [10, 1000],

    "rnngcn":{
        "rnn_layers": [1, 3],
        "gcn_layers": [1, 3],
        "cell_type": ["gru", "lstm"],
        "rnn_dropout": [0.0, 0.5],
        "gcn_dropout": [0.0, 0.5]

    },
    "grugcn":{
        "norm": ["mean", "gcn", "asym", "none"],
        "enc_layers": [1, 3],
        "gcn_layers": [1, 3]

    }
} 